\documentclass{article}
\usepackage{amsmath}

\title{Progress Report}
\author{Annabelle Cloutier}


\begin{document}
\maketitle

\tableofcontents

\section{Paper}

\subsection{Network Structure}

All of the below is based on the work in \cite{eco-dqn}
The network structure is incredibly generic most of the way through, but notes are written here to avoid having to parse through the equations that describe the network structure in the paper. Note that at any step where learned weights are used, the ReLU function is used on the resulting vector. The only exception is for the last set of learned weights in the readout layer.

\subsubsection{Node Embedding}\label{node-embedding}

The MPNN (Message Passing Neural Network) structure used converts each vertex into a set of $m$ observations and encodes them into a $n$-dimensional embedding using learned weights. The paper states that they use 64 dimensional embeddings and they do in code, but this could be any size. 

The observations in the paper as well as their justifications for them are as follows: 

\begin{enumerate}
    \item Whether the current vertex belongs to the solution set $S$ or not; 
    
    This is a local observation. The reason they state this observation is important is that it provides useful information for the agent to make a decision on whether the vertex should be added or removed from the solution set.

    \item The immediate cut change if the current vertex state is changed;
    
    This is also a local observation. Just as with the above observation, they state this provides useful information for decision making. They also state is that it allows the agent to exploit having reversible actions. This makes intuitive sense, as knowing the difference between having a vertex in the solution or not can allow the agent to make an informed decision on whether to add or remove it, or in this agent's case, possibly reversing an action. 

    \item The number of steps since the current vertex state was changed;
    
    Also a local observation. For this observation, they mention that it provides a simple history to the agent to prevent looping decisions. Because this observation gets larger as time goes on and a vertex is unchanged, the agent will be further incentivised to choose other vertices if it gets stuck in a loop choosing the same few vertices over and over. They also state it allows the agent to exploit reversible actions just as with observation 2. This specific observation could be important as the value increases as the vertex remains unchanged, which can entice the agent to choose vertices that have not been chosen in a long time to encourage exploration just as it will discourage loops.
    
    \item The difference of the current cut value from the best observed;
    
    This and all future observations are global. This observation they state ensures the rewards are Markovian. Just as with observations 2-4, this observation they also mention allows the agent to exploit having reversible actions, which makes intuitive sense. The agent knowing the difference between the current cut value and the best one observed, along with the other observations, can allow the network to make informed decisions on whether the current solution is a promising set to explore.

    \item The distance of the current solution set from the best observed;
    
    This observation as with observation 5 they state ensures the rewards are Markovian. Again, this observation allows the agent to exploit reversible actions. 

    The difference between this and observation 5 is that observation 5 compares the cut value only. This observation compares the solution sets and counts the number of vertices that differ between the best observed solution set of vertices and the current one. This is not explained in the paper, but can be seen in the code, specifically in src/envs/spinsystem.py, in the SpinSystemBase class' step function.

    Example for clarity:

    Best seen bitmask of vertices: $[0, 0, 1, 0, 1]$

    Current bitmask of vertices: $[0, 1, 1, 0, 1]$.

    Difference: 1. 

    Counting the number of different vertices they do in code by subtracting the two and counting the number of non-zero values. This can also be done using a bitwise XOR and doing a sum on the resulting bitmask. 

    \item The number of available actions that immediately increase the cut value;
    
    Once again, this observation ensures Markovian rewards, they also mention that this allows exploitation of reversible actions, which is more easily understandable.

    \item The number of steps remaining in the episode
    
    The only reasoning they have for this observation is that it accounts for the finite time used for solution exploration. 

\end{enumerate}

\subsubsection{Edge Embedding}

The edges for each vertex are also encoded into a separate $n$-dimensional embedding, same size as for each vertex, once again learned. The input for this step is the set of $m$ observations of the neighboring vertices catenated with the weight on the connecting edge, creating an $m + 1$ dimensional vector for each neighboring vertex. All of these vectors are then summed and passed through a learned layer, creating an $n - 1$ dimensional vector. At this stage, the resulting $n - 1$ dimensional vector is divided by the number of neighbors, catenated with the number of neighbors, and passed through another learned layer, resulting in an $n$-dimensional embedding representing the edges for a vertex.

The end result of these two steps are an $n$-dimensional embedding representing a vertex and another representing it's neighbors, all created using the same learned weights for every vertex. Meaning there will be $2|V|$, $n$-dimensional embeddings. 


\begin{enumerate}
    \item Why 64 dimensions on the vertex and edge embeddings? 
    
    It's nice that it's 64 but finding the reason why will be important to make informed decisions on modifying the network.
\end{enumerate}

\subsubsection{Message Passing}

Here there is a message pass layer and an update layer. The message pass per vertex is summing the products of the connected vertices and the weight, divided by the number of neighbors, catenated with the edge embedding for that vertex and then passed through learned weights, resulting in a $n$-dimensional vector.

The next is the update layer, which is the embedding for that vertex, catenated with the message and passed through another set of learned weights, into an $n$-dimensional vector, representing the "new" embedding for that vertex. 

The message then update is performed $K$ times. Mentioned in the paper and corroborated in the code, this is done 3 times, but can be done however many times necessary.

\begin{enumerate}
    \item Why have new network layers for these steps?
    
    As with the decision on the hidden layers sizes, understanding the justification for why certain steps are passed through learned functions before more calculations are performed will help make informed decisions on network changes.
\end{enumerate}

\subsubsection{Readout}

The readout layer goes through each vertex, summing the embeddings for the neighbors of that vertex, dividing the result by the number of vertices in the entire graph and then passing it through learned weights, resulting in an $n$-dimensional vector.

The embedding for the vertex itself is then catenated to the resulting vector and passed through another set of learned weights (without applying ReLU), giving in a single output value. This value represents the Q-value for that vertex, which is used by the algorithm to determine which vertex will be added/removed from the solution set on that step. The vertex associated to the maximum Q-value is added to the solution set if it doesn't yet belong to it, or removed from the solution set if it belongs to it.

\subsection{Reward Shaping and Training}

\subsubsection{Q Function, Q values and Training}

The Q function is an idea derived from Q-learning \cite{qlearning} which proposes a way for an agent to learn how to behave in an environments. It is defined as the expected value of the discounted sum of future rewards for any state-action pair of an environment. When an optimal Q function is derived, the agent chooses which action to perform in a certain state by selecting the state-action pair which corresponds to the largest Q-value, which is expected to give them to largest reward. The equation following equation represents this idea:
 
$Q^\pi(s, a) = E[\sum_{t=0}^{\infty} \gamma^t R(s_t) | s_0 = s, a_0 = a, \pi]$

where $\pi$ is a policy, mapping a state to a probability distribution over the actions, $R(s)$ is the reward for a given state and $\gamma^t$ is a discount given to change whether the agent prefers immediate or future rewards.

This Q function is learned by a Markov decision process. The agent traverses the environment and rewards are given at each step depending on the result of the action the agent performs.

Trying to find this Q function was known to be unstable or even diverge when nonlinear function approximators, like neural networks, were used to try and represent it \cite{td-func-approx}. In \textit{Mnih et al.}, they propose an approach to Q learning with two main ideas; namely experience replay and an iterative update, adjusting the Q values towards target values only periodically \cite{deepmind_2015}. The agent during training only chooses the actions associated with its current policy with probability $1 - \epsilon$ and otherwise chooses a random action. They demonstrate experimentally that with these ideas, they were able to train a model that had significant performance improvements compared to other existing models on 49 different Atari games.

These ideas are replicated in the training of ECO-DQN. For every episode in the training phase, a random graph is sampled with a random solution set. Then, for each time step in that episode, the agent chooses a random vertex based on the existing Q function with a probability $\epsilon$ and a vertex dictated by it's Q function otherwise. Then, the starting state, chosen vertex, reward and resulting state are added to the experience replay memory. Finally, after some fixed set of time steps, the network is updated by stochastic gradient descent on a minibatch sampled from the experience replay memory.

\subsubsection{Reward Shaping}

The reward in a certain state is given by the difference between the cut value in that state and the highest cut value seen so far in the episode, divided by the number of vertices. If the difference is negative, it's instead set to 0. The justification behind this choice is that a negative reward will discourage the agent from exploring states that give an immediately worse cut value, even if other cuts including that change later on may give better cut values. If this were to happen, it would discourage the agent from exploring different cut values and cause it to stay in a very small space near a locally maximal cut value. Because previously seen optimal states are stored in memory, it's beneficial to later let the agent explore more states in an attempt to find more locally optimal states, even if not always better than the previously seen local optimum.

They also define a reward for reaching locally optimal states of $\frac{1}{|V|}$. This is once again to encourage exploration. Because the previously seen best cut is stored, it's beneficial for the agent to explore other states, even if other local optimums may be worse than the previously found best cut. They state that local optima in combinatorial problems are typically close to each other and therefore by giving rewards for reaching new local optimums, the agent learns to hop between them during training as it is rewarded for this behaviour. Without this, they state that because there are far too many states to be visited in a finite time, which is typical for combinatorial optimization problems, it is therefore useful to focus on these subsets of states near local optimums.

There is no exact reason stated for the choice of value, however it can be hypothesized that if a constant value is chosen, this could cause disproportionate reward shaping on different sized graphs. For example, graphs that have significantly more locally optimum cuts could end up rewarding the network too much for imply finding local optimums instead of attempting to find a global optimum, while using the exploration of local optimums as a means to that end. Larger graphs are likely to have more locally optimal states, it therefore makes sense to choose a value that decays as the size of the graph increases. Therefore a reward proportional to the inverse of the number of vertices in the graph therefore makes the most sense. 

The choice to not randomly change states when a new local optimum is found appears to be mostly a choice that the authors made intentionally as they want the network to find some space that has numerous locally optimum states near each other to explore, and the best way to do this is to encourage finding nearby local optimums instead of sending the agent in different random directions every time a new local optimum is found. They do state that because there are far more states than can be visited within a finite time period, it's therefore more useful to find some local optimums that are near each other and explore that specific space of possible solutions.

They demonstrate this behaviour by observing the probability during any given timestep that the agent will either revisit a state, find a locally optimal state or find the maximum cut on the validation set. They show that as the number of timesteps goes up, the probability that the agent revisits a state goes up, as does the probability of finding the maximum cut, while the probability that the agent finds a local optimum goes up very quickly and stabilizes for the rest of the time steps, showing that the agent picks a certain set of local optimum and explores that space by revisiting previously seen states.

One thing this paper does not tackle is the issue of invalid solution states and how this would affect reward shaping, especially in a situation where the agent would be allowed to revert previously made actions. This isn't necessary for the Maximum Cut Problem, as any partition of the graph into two containing all vertices is a valid cut, but problems like the Traveling Salesman, Minimum Vertex Cover, Minimum Bisection  could possibly include states that are not valid solutions by either not being a complete path, not covering every vertex, having two sets of different sizes or having items that are not in a bin, respectively for each problem.

\subsection{Discussion on Generalization}

Most of the internal structure is generic for any graph, however because of the large number of changes that would likely need to be made to observations as well as the interpretation of the output for many other types of problems, it is likely that some changes to the internal structure would have to be made for the agent to make more educated decisions on new problems.

Everything outside of observations (input) and Q-values (output) only propagates information throughout the graph. This means the internal structure likely could remain mostly the same for any problem, so long as good decisions are made for the observations and that the output or it's interpretation are modified to fit new problems.

It is possible for more complex problems that some of the internal structure for the message and update sections of the network may have to change in order to accommodate for extra information. It is also possible that global observations could be embedded somewhere within the internal structure of the network instead of as input observations for every vertex.

The main issue comes with the output and it's interpretation. Each vertex is represented by a single value as the output, and that value is interpreted as adding or removing it from the solution set, in the context where the solution is a subset of the vertices in the graph. More specifically for the Max Cut problem, the vertex associated with the maximum Q-value (output value) is taken and then either added or removed from the solution set, depending on whether it already belongs to it or not. 

This approach works fine for a problem like Maximum Cut or Minimum Vertex Cover where the solution can be represented as a set representing chosen vertices for the solution, however any extra constraints forces the output interpretation to be completely redesigned.

For example, the Traveling Salesman Problem where the solution is an ordered list of vertices would not correctly work as the current implementation of simply adding or removing a vertex from the solution could not work.

The Minimum K-Cut Problem which can have an arbitrary number partitions of the graph would also not work with the current model as it would require extra decision making on deciding which set to move a vertex to.

The Minimum Bisection Problem can be represented as a set of chosen vertices, however the extra constraint that the number of chosen vertices has to be the same as the number of unchosen vertices makes the current output interpretation incomplete for solving this problem.

\subsection{Benchmarks}

The paper displays the performance of the graph in reference to S2V-DQN, a similar paper where the algorithm does not allow for reversing actions as well as a greedy algorithm. It also compares it's performance against modifications of itself, namely where some observations are restricted, intermediate rewards are not given for reaching locally optimal solutions, as well as keeping it from reversing its actions. 

They use GSet graphs G1-10 and G22-32 for calculating the approximation ratio, as well as the Physics dataset. 

\subsection{Graph Generation}

They train and test on Erdos-Renyi \cite{erdos} and Barabasi-Albert \cite{albert} graphs.

\section{Code}

The code holds the capability of running all of the above tests, but they're not explicitly written and must be generated via self-written code.

\subsection{Running Code}

They very generously provide a README file that specifies the exact commands to run in order to train, test and validate networks. However, there is no code for reproducing their specific tests. These all need to be hand-coded. The only results I've reproduced so far are training and testing.

\subsection{Graph Generation}

Unsure of the exact implementation, it seems they use NetworkX's ability to generate random graphs in order to do this. It's completely unnecessary though seeing as they have many test, validation and benchmark graphs pre-built within their code that can be used. If absolutely necessary, creating random graphs and storing them within a pickle file as they do would likely lower the workload on inputs as the code for parsing through these types of files is already prebuilt.

The code for generating new random graphs exists in src/envs/utils. This includes the code for generating Erdos-Renyi and Barabasi-Albert graphs, as well as classes for numerous other types of random graphs.

\subsection{Input Conversion}

The code for converting graphs into observations seems to exist within the file src/envs/spinsystem.py but further code inspection is needed to see exactly how this information is represented and decoded by the agent.

\subsection{Benchmarks}

They generously provide testing, validation and benchmark graphs in Pickle files, which is a special type of object file for Python, as well as their solutions, which are also stored in Pickle files, but are simply a boolean (technically floats, just 1.0 and 0.0) list for determining whether a vertex is in the solution set or not.

\section{Minimum Vertex Cover}

A vertex cover is a set of vertices such that every edge in a graph has at least one endpoint in the that set of vertices. The vertex cover of smallest magnitude for a graph is known as the Minimum Vertex Cover. Finding this cover is known to be NP-hard. To tackle this issue with the approach in ECO-DQN, a few problems need to be solved:

\begin{enumerate}
    \item What observations will be used for each vertex 
    \item What will the reward be for finding a valid solution 
    \item What will be the punishment (if any) for finding sets that are invalid solutions 
    \item Should an intermediate reward be used for finding valid solutions that are locally optimal
    \item Should we allow invalid candidate solutions
\end{enumerate}

Thankfully, some of these are somewhat trivial to solve, like the observations and reward, by either using ideas from ECO-DQN \cite{eco-dqn} and the similar S2V-DQN \cite{s2v-dqn} which does not allow for exploration/undoing actions but does tackle the Minimum Vertex Cover.

The last question we'll tackle in two parts. First, we'll attempt this problem by giving the network valid solutions and not allowing it to select invalid ones by severely penalizing those actions and randomizing a new solution to give it at test time to avoid the network getting stuck in local optimums in case it continuously tries to suggest invalid candidates.

\subsection{Observations}

In ECO-DQN, they have their set of observations from which we can draw to design the observations for the Minimum Vertex Cover. Some of these are even somewhat trivial to convert, namely:

\begin{itemize}
    \item Observation 1, "Vertex sate, i.e. if $v$ is currently in the solution set, $S$" is very easily translatable. We can directly copy this information, irrespective of if the current state is a valid solution.
    \item Observation 3 "Steps since the vertex state was last changed". This one also can be directly copied.
    \item Observation 4 "Difference of current cut-value from best observed". We will translate this to the difference between the current cover set size and the best observed.
    \item Observation 5 "Distance of current solution set from the best observed". In the paper they do not define this, but it is explained through their implementation in Section \ref{node-embedding}, Node Embeddings. We can also use this information for the Minimum Vertex Cover in the same way.
    \item Observation 6 "Number of available actions that immediately increase the cut value" can also be translated. Namely, counting the number of actions (or vertices) that when removed from the set will reduce the number of vertices in the cover. 
    \item Observation 7 "Steps remaining in the episode". This, once again, can be directly copied.
\end{itemize}

The observation that will change is the immediate cut change. Due to the nature of the Minimum Vertex Cover, the immediate change in the cover set size is implied by observation one. Therefore, we will change this slightly to instead represent an observation for in the case where we'll allow invalid candidates: 

\begin{enumerate}
    \item Difference of current edge cover from the best observed. The "edge cover" in this case will be the number of edges covered by the state, which can be less than the total number of edges in the graph.
\end{enumerate}

We also want to embed information about the validity of the current state in case we allow invalid candidates, therefore we also add the following observations: 

\begin{enumerate}
    \item Immediate change of edge cover on vertex change, which will compute the difference of the number of edges that are covered by the state for every vertex flip
    \item Whether the current cover is valid; that it covers all edges.
\end{enumerate}

In the end, we'll have 9 observations, some of them only used for when the network will be permitted to explore candidates that are invalid solutions. Observations 1-6 will be used regardless of whether we allow invalid candidates, observations 7-9 will only be used if we allow invalid candidates.

\begin{enumerate}
    \item Vertex state i.e. whether the vertex is in the current solution set.
    \item Steps since the vertex state was last changed.
    \item Difference of current cover set size from the best observed.
    \item Distance of current solution set from the best observed. 
    \item Number of available actions that immediately decrease the number of vertices in the solution.
    \item Steps remaining in the episode.
    \item Immediate change in current edges covered.
    \item Difference of current edges covered from best observed.
    \item Whether the current cover is a valid cover.
\end{enumerate}

In this case as with ECO-DQN \cite{eco-dqn}, we have local (1-3, 7) and global (4-6, 8-9) observations.

To start, we'll do a basic algorithm to select edges at random, picking the nodes incident on the edge and removing all edges incident on the chosen nodes until all edges are covered. We will then use this generated solution and give it to the agent as a starting point. In this scenario, we won't allow for invalid candidates, and will give a large negative reward for choosing invalid candidates. If an invalid choice is suggested during test time, we'll generate another random valid solution and get the network to continue from the newly generated solution. 

Where we'll allow for invalid candidates, we'll start with an empty solution set as the first candidate.

\subsection{Reward Shaping}

Due to the difference between how proposed solutions are going to be in the Maximum Cut and Minimum Vertex Cover, some different rewards are going to be required to adequately represent the problem.

In ECO-DQN \cite{eco-dqn}, the reward for a certain state is framed as

$R(S) = max(0, C(S) - C(S^*)) / |V|$

where $C(S)$ is the cut value for state $S$ and $S^*$ will be the previously best seen cut value. They also grant intermediate rewards $\frac{1}{|V|}$ any time a new locally optimal cut is found, which is a cut that has not yet been seen where any change to a vertex reduces the value of the cut.

In S2V-DQN \cite{s2v-dqn}, the reward for a Minimum Vertex Cover they define as $R(S, v) = -|S| - -|S'|$ being the change in their cost function when going from state $S$ and adding vertex $v$ to it, resulting in state $S'$. In these cases, the state is the set of vertices chosen for a candidate solution. We can reformulate their idea slightly to coincide with the idea in ECO-DQN to allow for exploration by instead looking only at defining the reward on a specific state in comparison to the best seen. 

In our case, the best seen is not so simply defined. Because constructed candidates may not be valid, some way to compare invalid candidates to valid ones has to be devised such that we can choose a previous "best" candidate to use as a comparison for future candidates. What we want is a reward mechanism that allows valid candidates to always be chosen as the best over invalid ones, as well as for valid candidates to be compared to each other in such a way that a candidate that improves the solution gives a better reward than ones that don't. Similarly for invalid candidates, we'd like for them to be compared to each other in such a way that an invalid candidate that is closer in some measure to being a valid solution gives a better reward than one that is further away. One way to do this is to ensure that valid solutions gives positive rewards increasing in magnitude depending on it's quality, and for invalid solutions to give negative rewards increasing in magnitude depending on it's relative lack of closeness to being a valid candidate.

With this in mind, we can define this idea of the "score" of a candidate solution. This score will be used to compare different candidates in the same way that is done in ECO-DQN so valid solutions will always be prioritized. This score function is defined as follows:

$SC(S) = V(S) * SQ(S) - DI(S)$

$SC(S)$ is the score of the solution that will be used to compare it to other solutions, and more importantly the previous best seen for determining whether this new solution should take precedence as the new best seen and for calculation for the reward. This can also be used to determine whether we are in a local minimum. $V(S)$ is a validity bit. Therefore $V(S)$ is defined as follows:

\[
    V(S) =
    \begin{cases}
        1,&\text{if S is a valid candidate}\\
        0,&\text{otherwise}
    \end{cases}
\]

$SQ(S)$ is a function defining the value of a solution such that it is larger for a better solution. Intuitively this is the solution's quality. For the Minimum Vertex Cover, this would be the number of vertices not in the cover, $SQ(S) = |V| - |S|$. This way a smaller solution set will have a larger score.

$DI(S)$ is the degree of invalidity. This function should be larger the farther away from a valid solution the candidate is. This function should always evaluate to zero for a valid candidate. For the Minimum Vertex Cover, this degree of invalidity can be easily defined by counting the number of edges not covered by the candidate solution. Therefore for the Minimum Vertex Cover, $DI(S) = UncoveredEdges(S)$. 

This score function guarantees that valid solutions have a positive score by making it the only existing term and also guarantees that invalid solutions are granted negative scores by ignoring the solution's quality by the validity bit and increasing the magnitude for solutions that require more actions to render valid. 

For computing a reward, we'll use the normalized $SQ(S)$ and $DI(S)$ functions, $NormSQ(S)$ and $NormDI(S)$ where we normalize each to be bounded in $[0, 1]$. For the Minimum Vertex Cover this means dividing by the number of vertices in the graph and for $DI(S)$ we divide by the number of edges in the graph. This new function is $NormSC(S) = V(S) * NormSQ(S) - NormDI(S)$. Therefore we can define the reward in much the same way as DQN:

$R(S) = max(0, NormSC(S) - NormSC(S^*))$

Note that for the maximum cut, this evaluates to the same function used in ECO-DQN because $DI(S) = 0$ and $V(S) = 1$ for the maximum cut. However, this new score function allows us to tackle other problems that have more constraints. This function also allows us to easily find local optimums as the score for flipping other states when compared to a local optimum will always give a negative or zero reward if no better valid solutions are able to be found with one action. The term comparing the normalized scores should also guarantee the following:

\begin{enumerate}
    \item Invalid $S$ compared to invalid $S^*$ only give positive rewards when $S$ is closer to being valid than $S^*$. 
    \item Invalid $S$ compared to valid $S^*$ will never give a positive reward. 
    \item Valid $S$ compared to invalid $S^*$ will always give a positive reward. 
    \item Valid $S$ compared to valid $S^*$ will only give positive rewards if $S$ is a better solution than $S^*$. 
\end{enumerate}

This is true for the way we defined our function with the Minimum Vertex Cover. One thing to note is that defining how close we are to a valid solution can be a difficult problem if we define it as the minimum number of actions required to get to a valid state; for the Minimum Vertex Cover, having 2 uncovered edges may only require one action to solve, but could also require two actions to solve depending on the state of the solution set. However, determining the minimum number of actions required to get to a valid solution is just solving the problem excluding the current nodes in the set. Therefore some compromise has to be made. This problem does not exist in the Minimum Bisection for example, where we simply measure the size difference between the two sets, which gives a direct measure for how many actions are required to make a valid solution.

Further, because we're once again using this maximum function, this means the agent doesn't get punished for exploring worse states. This reward function will actually evaluate valid and invalid candidate solutions during exploration evenly when the best observed is a local optimum, meaning it can explore in either direction to find new valid candidates. 

As with ECO-DQN, we'll also give intermediate rewards for finding new local optimums in order to avoid situations where rewards become incredibly scarce and to encourage it to find new locally optimal states to explore. 

For the version of this algorithm where we won't allow for invalid candidates, we'll use this same reward function during training and testing. However, during both, we'll start from a valid candidate and if invalid candidates are suggested, we'll randomize into a new valid candidate. 

The last thing of note is that this reward system also allows us to represent S2V-DQN in the same way and disallowing exploration by starting with an empty candidate and progressively building up the solution, not allowing the network to undo a previously flipped vertex. This means the previously seen optimum will always be the previous state. In our case, the reward at each step is based on how many more edges get covered by the new state instead of only giving the same reward at each step. This may seem greedy and like it would give the same cumulative reward because all valid covers will give the same cumulative reward if you're only counting edges. However, because the end result is a valid solution, the function completely changes in the final step and the cumulative reward will be larger for finding better covers. Therefore, by having a larger reward horizon in the Q function evaluation, which the ECO-DQN paper already does with a $\gamma$ of 0.95, we can guide the network to prefer future rewards more, meaning it will be more likely to prefer smaller covers as it gets more reward for this behavior in the long term. An added benefit of this is that if the function approximation that the network generates is accurate, these larger rewards should be closer to the initial empty candidate and should therefore be easier to find. 

\subsection{Training}

Training this network will be algorithmically identical to ECO-DQN using the ideas from Deep Q-Learning described in \textit{Mnih et al.} \cite{deepmind_2015}.

\subsection{Implementation and Results}

Notes for when I start implementation: 

\begin{itemize}
    \item Still need to find Integer Program for comparison. Do this last. It's useless to have one if I can't even get the above working.
    \item Start with basics: do basic solution of MVC then use the network to refine (i.e. don't allow invalids)
    \item Because of the generalized nature of it all, consider training on max cut/min cover and then testing on the different problem to see how well the network generalizes. 
\end{itemize}

\subsection{Generalization}

\textbf{Just some footnotes/brainstorming for now}

To generalize these ideas to any problem, some framework needs to be solidified for choosing observations and rewards. Something related to the constraints of the problem (if any) would need to be specified. For Minimum Vertex Cover there's a simple way of doing this as described in this section, but extending this to other problems is much more difficult. Further, there's no real way that I know of that could actually confirm whether other frameworks for learning would actually yield good results other than manually verifying it by creating the observations and reward mechanism then training a network (obviously bad and not very useful).

If the framework I thought of for MVC works as well as the different framework for MaxCut did, I'd say there's at least some merit to the general structure and idea of ECO-DQN and the idea of encoding some reward and even extra observations to encode validity constraints. It's a matter of formulating observations and rewards so they can encode not only the actual problem constraint (i.e. cut size for max cut or minimum bisection, vertex cover size for minimum vertex cover, path length for traveling salesman), but also some sort of validity constraint (reward to determine whether a candidate is a valid solution) that can generalize to different problems (i.e. size of partition for minimum bisection, whether a cover actually reaches every edge for minimum vertex cover, whether a path is a full tour for traveling salesman).

Many of the constraints would likely need to be formulated such that they change depending on whether the problem is maximization or minimization. For example, choosing $max(0, cover(S) - cover(S^*))$ for MVC only makes sense because you're trying to maximize the number of edges covered. When $S^*$ is a valid candidate, this term will always be 0. If you're instead trying to minimize something, the terms would have to be swapped. For example, in the minimum bisection, you'd want to minimize the absolute difference betweeen the size of the partitions of the graph. Therefore, the idea for that problem would be more like $max(0$, partdiff$(S^*) - $partdiff$(S))$ for a new candidate. Note that taking the absolute difference doesn't work because we actually want negative values to exist to reach 0.

Definitely problems arise if the decision isn't binary, adding or removing a vertex from a single solution set, like with Minimum K-Cut.

\bibliography{main}
\bibliographystyle{ieeetr}


\end{document}